{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cbba39-d8c8-4223-84cb-a72a48c17537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "026a4c00-5dad-4b1a-a56d-d1f746de7778",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/vscode/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cd8e7ad-9f87-4ad9-9fd6-f0985b06cf66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.293, 'neu': 0.544, 'pos': 0.163, 'compound': -0.34}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example Test\n",
    "analyzer.polarity_scores(\"Hais I really wish I was in NUS. So tired of literally no university life.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539635d8-8ea5-4fdf-aab9-57db5e542db5",
   "metadata": {},
   "source": [
    "### Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e24158b0-973e-4648-bc37-534a292d7e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View raw html\n",
    "# view-source:file:///Users/gin/Downloads/Telegram%20Lite/ChatExport_2025-04-27/messages.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b96344c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4173/3350552588.py:31: UserWarning: Parsing dates in %d.%m.%Y %H:%M:%S UTC%z format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  dt = pd.to_datetime(datetime)\n",
      "/tmp/ipykernel_4173/3350552588.py:144: UserWarning: Parsing dates in %d.%m.%Y %H:%M:%S UTC%z format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df['datetime'] = pd.to_datetime(df['datetime'])\n"
     ]
    }
   ],
   "source": [
    "# prepare the data, convert to 'confess_data.csv'\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "\n",
    "data = []\n",
    "x = 1 # for poll counter\n",
    "\n",
    "# Loop through all HTML files in a folder\n",
    "for filename in glob.glob('smu_confess_data/*.html'):\n",
    "    # if filename == 'smu_confess_data/messages38.html': # look at the first file only for now\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        soup = BeautifulSoup(file, 'html')\n",
    "\n",
    "        # go by the hierarchy - get everything that has div class \"text\", displayed as a list\n",
    "        div = soup.find_all('div', class_='body')[1:] # remove the first item which has class \"text bold\"\n",
    "        \n",
    "        # go through each confession\n",
    "        for text_chunk in div:\n",
    "            # if need to print the text chunk\n",
    "            # print(\"\\n--------------TEXT CHUNK--------------\")\n",
    "            # print(text_chunk)\n",
    "\n",
    "            # GET DATETIME\n",
    "            datetime = text_chunk.find('div', class_='pull_right date details')\n",
    "            if datetime:\n",
    "                datetime = datetime.get('title')\n",
    "                # split datetime into date and time\n",
    "                dt = pd.to_datetime(datetime)\n",
    "                \n",
    "                date = dt.date()\n",
    "                time = dt.time()\n",
    "                day_of_week = dt.day_name()\n",
    "                \n",
    "                # print('Datetime:', datetime)\n",
    "\n",
    "            # GET CATEGORY, ID\n",
    "            # there are 2 or more instances of \"return ShowHastag\" - get everything and index from there\n",
    "            category_and_id = text_chunk.find_all('a', onclick=re.compile(r'ShowHashtag')) \n",
    "            # print(category_and_id)\n",
    "\n",
    "            category_wo_hashtag = 'null'\n",
    "            ref_id_wo_hashtag = 'null'\n",
    "            id_wo_hashtag = 'null'\n",
    "            total_votes = 'null'\n",
    "            \n",
    "\n",
    "            # 1. normal confessions - have 2 hashtags\n",
    "            if len(category_and_id)==2:\n",
    "                \n",
    "                # Get category\n",
    "                category = category_and_id[0].text\n",
    "                category_wo_hashtag = category[1:]\n",
    "                # print('Category:', category_wo_hashtag) # remove the hashtag\n",
    "                \n",
    "                # Get id\n",
    "                id = category_and_id[1].text\n",
    "                id_wo_hashtag = id[1:]\n",
    "                # print('ID:', id_wo_hashtag)\n",
    "\n",
    "                # GET CONFESSION\n",
    "                text = text_chunk.get_text(separator=' ', strip=True) # found out that could get all the text in text_chunk like this too\n",
    "                text = text.replace(category, \"\").replace(id, \"\").replace(\"SMU Confess\", \"\") # replace the category, id, also \"SMU Confess\" that sometimes appears\n",
    "                confession = text.split(\"|\")[0][9:].lstrip().rstrip()\n",
    "                # print('Confession:', confession)\n",
    "\n",
    "                # Set type\n",
    "                type = 'confession'\n",
    "\n",
    "                # add to data\n",
    "                data.append([id_wo_hashtag, ref_id_wo_hashtag, category_wo_hashtag, confession, total_votes, datetime, date, day_of_week, time, type])\n",
    "\n",
    "            # 2. confession replying to another confession, will have more confession ids - have 3 hastags\n",
    "            elif len(category_and_id)==3:\n",
    "\n",
    "                # Get category\n",
    "                category = category_and_id[0].text\n",
    "                category_wo_hashtag = category[1:]\n",
    "                # print('Category:', category_wo_hashtag)\n",
    "\n",
    "                # Get previous confession reference id\n",
    "                ref_id = category_and_id[1].text\n",
    "                ref_id_wo_hashtag = ref_id[1:]\n",
    "                # print('Confession Reference ID:', ref_id_wo_hashtag)\n",
    "                \n",
    "                # Get id\n",
    "                id = category_and_id[2].text\n",
    "                id_wo_hashtag = id[1:]\n",
    "                # print('ID:', id_wo_hashtag)\n",
    "\n",
    "                # GET CONFESSION\n",
    "                text = text_chunk.get_text(separator=' ', strip=True) # found out that could get all the text in text_chunk like this too\n",
    "                text = text.replace(category, \"\").replace(id, \"\").replace(ref_id, \"\").replace(\"SMU Confess\", \"\") # replace the category, id, also \"SMU Confess\" that sometimes appears\n",
    "                confession = text.split(\"|\")[0][9:].lstrip().rstrip()\n",
    "                # print('Confession:', confession)\n",
    "\n",
    "                # Set type\n",
    "                type = 'confession'\n",
    "\n",
    "                # add to data\n",
    "                data.append([id_wo_hashtag, ref_id_wo_hashtag, category_wo_hashtag, confession, total_votes, datetime, date, day_of_week, time, type])\n",
    "\n",
    "            # 3. others - people copy pasting the entire confession, resulting in more confession ids / polls - more than 3 or no hashtags\n",
    "            else:           \n",
    "\n",
    "                # IF IT IS A POLL\n",
    "                # Get confession\n",
    "                if text_chunk.find('div', class_='media_poll'):\n",
    "\n",
    "                    # set an id for the polls since there is no official id in the confessions  \n",
    "                    # if len(str(x)) < 8:\n",
    "                    #     str_x = ((8-len(str(x))) * '0') + str(x)\n",
    "                    id_wo_hashtag = f\"P{int(x):08d}\" # makes the id 8 digits\n",
    "                    x += 1\n",
    "                    \n",
    "                    # get the poll text\n",
    "                    poll_div = text_chunk.find('div', class_='media_poll')\n",
    "                    confession = poll_div.get_text(separator=' ', strip=True)\n",
    "                    # print(confession)\n",
    "\n",
    "                    # get the total votes\n",
    "                    total_votes = poll_div.select_one('div.total.details').get_text(strip=True)\n",
    "                    # print(total_votes)\n",
    "\n",
    "                    # need to remove the total votes from the poll text\n",
    "                    confession = confession.replace(total_votes, \"\")\n",
    "                    # print(confession)\n",
    "\n",
    "                    # Set type\n",
    "                    type = 'poll'\n",
    "\n",
    "                    # add to data\n",
    "                    data.append([id_wo_hashtag, ref_id_wo_hashtag, category_wo_hashtag, confession, int(total_votes.replace(' votes', '')), datetime, date, day_of_week, time, type])\n",
    "\n",
    "\n",
    "                # IF IT HAS MORE THAN 3 HASHTAGS\n",
    "                continue   \n",
    "                             \n",
    "\n",
    "# collate into csv, sort by datetime\n",
    "df = pd.DataFrame(data, columns=['id', 'ref_id', 'category', 'confession', 'total_votes', 'datetime', 'date', 'day_of_week', 'time', 'type'])\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "df = df.sort_values(by='datetime', ascending=True)\n",
    "df.to_csv('confess_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6735ad76",
   "metadata": {},
   "source": [
    "### Vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8184fa-32bf-4a8a-bebd-edac60387b09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# perform vader analysis on 'confess_data.csv' - not all rows were able to be analysed\n",
    "# save it as 'confess_data_vader.csv'\n",
    "\n",
    "# Read the csv\n",
    "confess_df = pd.read_csv('confess_data.csv')\n",
    "display(confess_df)\n",
    "vader_results = {}\n",
    "\n",
    "# Apply vader\n",
    "for i, row in confess_df.iterrows():\n",
    "    confession = row['confession']\n",
    "    id = row['id']\n",
    "    if type(confession) != float: # some confessions cannot be analysed as they are NaN, which is float data type\n",
    "        vader_results[id] = analyzer.polarity_scores(confession) # perform vader sentiment analysis on confession\n",
    "    \n",
    "# create df to store the analysis results\n",
    "vader_df = pd.DataFrame(vader_results).T\n",
    "display(vader_df)\n",
    "\n",
    "# combine the dfs\n",
    "# confess_df = confess_df.join(vader_df)\n",
    "confess_df_vader = pd.merge(confess_df, vader_df, left_on='id', right_index=True, how='left')\n",
    "\n",
    "# wrangle datetime\n",
    "\n",
    "\n",
    "# create a new csv to store the vader df\n",
    "confess_df_vader.to_csv('confess_data_vader.csv', index=False)\n",
    "display(confess_df_vader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e457eec-ee3d-4921-9c3b-b26da508337e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform visualisation on the data\n",
    "!pip install plotly.express"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d040481b-0b9b-4923-b650-bb385e217a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "hist = px.histogram(confess_df_vader, nbins=20, x='pos')\n",
    "hist.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f6805b-e889-4bed-9e26-a34276a1fbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "bar = sns.histplot(data=confess_df_vader, bins=20, x='neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7cba43-7b67-4633-a280-8c9c0c08ed8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81dd4b5-ee2d-4b16-8ad8-d636d6e62393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508067d8-0ad8-4426-a7b2-6982e2ec0248",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3728a81a-d27d-4e87-af29-4ec7fcc31170",
   "metadata": {},
   "source": [
    "### ML - Multilingual Sentiment Analysis (doesn't work, max tokens exceeded, can only take in 512)\n",
    "https://huggingface.co/tabularisai/multilingual-sentiment-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5886975b-1b3a-466d-ac3c-b82ceaef67bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-classification\", model=\"tabularisai/multilingual-sentiment-analysis\")\n",
    "\n",
    "# Classify a new sentence\n",
    "sentence = \"I love this product! It's amazing and works perfectly.\"\n",
    "result = pipe(sentence)\n",
    "\n",
    "# Print the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee99de1-2a8d-4879-888b-aaff066c6d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cannot use the pipeline as it has restriction on max tokens\n",
    "\n",
    "# # Read the csv\n",
    "# confess_df = pd.read_csv('confess_data.csv')\n",
    "# display(confess_df)\n",
    "# ml_results = {}\n",
    "\n",
    "# # Apply vader\n",
    "# for i, row in confess_df.iterrows():\n",
    "#     confession = row['confession']\n",
    "#     id = row['id']\n",
    "#     if type(confession) != float: # some confessions cannot be analysed as they are NaN, which is float data type\n",
    "#         ml_results[id] = pipe(confession) # perform vader sentiment analysis on confession\n",
    "    \n",
    "# # create df to store the analysis results\n",
    "# ml_df = pd.DataFrame(ml_results).T\n",
    "# display(ml_df)\n",
    "\n",
    "# # combine the dfs\n",
    "# # confess_df = confess_df.join(vader_df)\n",
    "# confess_df_ml = pd.merge(confess_df, ml_df, left_on='id', right_index=True, how='left')\n",
    "\n",
    "# # create a new csv to store the vader df\n",
    "# confess_df_ml.to_csv('confess_data_ml.csv', index=False)\n",
    "# display(confess_df_ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d3b25b-e481-42ca-9e68-83e763b4c500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to use without pipeline to change the max_length of tokens\n",
    "# tried to tweak the max_length of tokens, but it cannot be extended, max can only be 512\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "model_name = \"tabularisai/multilingual-sentiment-analysis\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "ml_results = {}\n",
    "\n",
    "\n",
    "def predict_sentiment(texts):\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=2000)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    sentiment_map = {0: \"Very Negative\", 1: \"Negative\", 2: \"Neutral\", 3: \"Positive\", 4: \"Very Positive\"}\n",
    "    results = []\n",
    "    for probs in probabilities:\n",
    "        pred_idx = torch.argmax(probs).item()\n",
    "        pred_label = sentiment_map[pred_idx]\n",
    "        # Convert tensor to list and pair with label\n",
    "        score_dict = {sentiment_map[i]: float(probs[i]) for i in range(len(probs))}\n",
    "        results.append((pred_label, score_dict))\n",
    "    return results[0][1]\n",
    "\n",
    "# print(predict_sentiment('helloooo'))\n",
    "\n",
    "\n",
    "\n",
    "# Read the csv\n",
    "confess_df = pd.read_csv('confess_data.csv')\n",
    "display(confess_df)\n",
    "ml_results = {}\n",
    "\n",
    "\n",
    "# Apply ml\n",
    "for i, row in confess_df.iterrows():\n",
    "    confession = row['confession']\n",
    "    id = row['id']\n",
    "    if type(confession) != float: # some confessions cannot be analysed as they are NaN, which is float data type\n",
    "        ml_results[id] = predict_sentiment(confession) # perform vader sentiment analysis on confession\n",
    "\n",
    "ml_results\n",
    "\n",
    "\n",
    "# create df to store the analysis results\n",
    "ml_df = pd.DataFrame(ml_results).T\n",
    "display(ml_df)\n",
    "\n",
    "# combine the dfs\n",
    "# confess_df = confess_df.join(vader_df)\n",
    "confess_df_ml = pd.merge(confess_df, ml_df, left_on='id', right_index=True, how='left')\n",
    "\n",
    "# create a new csv to store the vader df\n",
    "confess_df_ml.to_csv('confess_data_ml.csv', index=False)\n",
    "display(confess_df_ml)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6d9b7a-177d-4a32-9a67-28de3ee1f1f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
